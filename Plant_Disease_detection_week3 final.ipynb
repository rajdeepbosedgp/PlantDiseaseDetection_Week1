{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "EF01Rp40ODtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LIqwZgZN5qb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Zip File"
      ],
      "metadata": {
        "id": "uQGMrR0rOQds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# ZIP file path\n",
        "zip_path = '/content/drive/MyDrive/archive (2).zip'\n",
        "extract_dir = '/content'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f'Successfully extracted: {zip_path}')\n",
        "    except zipfile.BadZipFile:\n",
        "        print('Error: Not a valid zip file.')\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "else:\n",
        "    print('Error: ZIP file does not exist.')\n"
      ],
      "metadata": {
        "id": "HUdJDRy3OSwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "SoGXeG1AOVN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "hOnHUM19OXHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Data Paths and Image Size"
      ],
      "metadata": {
        "id": "vqpjfbFjObju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
        "valid_path = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
        "img_size = 224\n"
      ],
      "metadata": {
        "id": "KknJlrW5Onli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Data Generator"
      ],
      "metadata": {
        "id": "ABti-TPsOrwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255.0,\n",
        ").flow_from_directory(\n",
        "    train_path,\n",
        "    batch_size=164,\n",
        "    target_size=(img_size, img_size),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "cYXFv5pzOvFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Data Generator"
      ],
      "metadata": {
        "id": "OhEuodzHOy49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255.0\n",
        ").flow_from_directory(\n",
        "    valid_path,\n",
        "    batch_size=164,\n",
        "    target_size=(img_size, img_size),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "MkmxZ43TO22E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Training Images"
      ],
      "metadata": {
        "id": "Vr_8An1-O6ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(train_generator.class_indices.keys())\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for x_batch, y_batch in train_generator:\n",
        "    n = min(16, len(x_batch))\n",
        "    for i in range(n):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(x_batch[i])\n",
        "        plt.title(classes[np.argmax(y_batch[i])])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    break\n"
      ],
      "metadata": {
        "id": "Zc7v4EYUO8aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build CNN Model"
      ],
      "metadata": {
        "id": "KcWcyTM9O-_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, 7, padding=\"same\", activation=\"relu\", input_shape=(224, 224, 3), name=\"Conv1\"),\n",
        "    keras.layers.MaxPooling2D(pool_size=2, name=\"Pool1\"),\n",
        "\n",
        "    keras.layers.Conv2D(64, 5, padding=\"same\", activation=\"relu\", name=\"Conv2\"),\n",
        "    keras.layers.MaxPooling2D(pool_size=2, name=\"Pool2\"),\n",
        "\n",
        "    keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"Conv3\"),\n",
        "    keras.layers.MaxPooling2D(pool_size=2, name=\"Pool3\"),\n",
        "\n",
        "    keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\", name=\"Conv4\"),\n",
        "\n",
        "    keras.layers.Flatten(name=\"Flatten\"),\n",
        "    keras.layers.Dense(128, activation=\"relu\", name=\"Dense1\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(64, activation=\"relu\", name=\"Dense2\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(38, activation=\"softmax\", name=\"Output\")\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "SUQVkcYqPBEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the Model"
      ],
      "metadata": {
        "id": "DbyLX_cZPD4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(),\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n"
      ],
      "metadata": {
        "id": "J-yfLx3uPHY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Callbacks"
      ],
      "metadata": {
        "id": "adPsPyKAPKMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_lr=1e-5)\n",
        "\n",
        "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n"
      ],
      "metadata": {
        "id": "_afgSHRGPL-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train the Model"
      ],
      "metadata": {
        "id": "192lAwLVPOSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=valid_generator,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQtwouyNPQm8",
        "outputId": "7f977236-0e75-4035-bcd2-5472bcc4a9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 36/429\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:43:58\u001b[0m 43s/step - accuracy: 0.0305 - loss: 4.0970 - precision: 0.0325 - recall: 0.0017"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Training Metrics"
      ],
      "metadata": {
        "id": "YFYTABE7PTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "precision = history.history['precision']\n",
        "val_precision = history.history['val_precision']\n",
        "recall = history.history['recall']\n",
        "val_recall = history.history['val_recall']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, acc, 'g', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kg9_5HlmPS2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "0OSnvHVQPXjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255.0\n",
        ").flow_from_directory(\n",
        "    valid_path,  # Assuming valid is used for testing\n",
        "    batch_size=64,\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_evaluate = model.evaluate(test_generator)\n",
        "\n",
        "print('Loss:', model_evaluate[0])\n",
        "print('Accuracy:', model_evaluate[1])\n",
        "print('Precision:', model_evaluate[2])\n",
        "print('Recall:', model_evaluate[3])\n"
      ],
      "metadata": {
        "id": "gnT78nD8PZ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model and Class Indices"
      ],
      "metadata": {
        "id": "b2NBihfcPb5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('PDDS.keras')\n",
        "\n",
        "import json\n",
        "with open('class_indices.json', 'w') as f:\n",
        "    json.dump(train_generator.class_indices, f)\n"
      ],
      "metadata": {
        "id": "-XRem-J2Pdnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}